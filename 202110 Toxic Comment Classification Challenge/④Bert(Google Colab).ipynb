{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiiE4PFCDkhV"
   },
   "source": [
    "# 1. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8JHOKcePBC1",
    "outputId": "15b65d9c-03fb-4d4d-d49a-7fe9b210cfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAAsZdV6O2s9",
    "outputId": "3aa89bc7-90a0-4808-a2ed-072475ffad28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PQI Taggle',\n",
       " 'train.csv',\n",
       " 'test.csv',\n",
       " 'Untitled.ipynb',\n",
       " '1601556865711.jpg',\n",
       " '1601556867726.jpg',\n",
       " '1601558074690.jpg',\n",
       " '1601992462229.jpg',\n",
       " 'Kaggle Driver Detection 手法説明.pptx',\n",
       " 'Kaggle Driver Detection 手法説明.pdf',\n",
       " 'toxic_comment.zip',\n",
       " 'Toxic Comment Classification Challenge.mm',\n",
       " 'train_preprocessing_lower.csv',\n",
       " 'test_preprocessing_lower.csv',\n",
       " 'train_preprocessing_upper_allfeature.csv',\n",
       " 'test_preprocessing_upper_allfeature.csv',\n",
       " 'submission.csv',\n",
       " '20201030ラズパイ講座 完成品.zip',\n",
       " 'Colab Notebooks',\n",
       " 'submission_result.csv',\n",
       " 'test_preprocessing.csv',\n",
       " 'train_preprocessing.csv',\n",
       " 'Naive Bayes.ipynb',\n",
       " 'submission_result_preprocessing.csv',\n",
       " 'submission_result_preprocessing_lower.csv',\n",
       " 'submission_result_preprocessing_upper_allfeatires.csv',\n",
       " 'submission1.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iu64v5fNOuv3",
    "outputId": "8ad4f040-a30c-471e-9e5d-0e2db7e9e595"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  ... identity_hate\n",
       "0  0000997932d777bf  ...             0\n",
       "1  000103f0d9cfb60f  ...             0\n",
       "2  000113f07ec002fd  ...             0\n",
       "3  0001b41b1c6bb37e  ...             0\n",
       "4  0001d958c54c6e35  ...             0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データの読み込み\n",
    "import pandas as pd\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test_labels = pd.read_csv('test.csv')\n",
    "df_test_labels = df_test_labels.set_index('id')\n",
    "\n",
    "df_submission = pd.read_csv('submission.csv', index_col='id')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqJDVAImDqs8"
   },
   "source": [
    "# 2. ライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1uPcwzPOfDb",
    "outputId": "0bf799b6-b3b3-4854-9237-fbcf399382df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.3.0 in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.1.94)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.16.14)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.14 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.19.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.14->boto3->transformers==2.3.0) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#ライブラリの読み込み\n",
    "!pip install transformers==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOpso-rvOTO2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pFehJseCHBz"
   },
   "source": [
    "# 3. 学習データの作成\n",
    "## 3.1 学習用データ作成の手順の確認\n",
    "### 3.1.1 input_ids(テキストをエンコードしたID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VsnQ4H-5b7k"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#学習済みモデルを指定\n",
    "#12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "#パラメータの指定\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "max_seq_len = 128\n",
    "\n",
    "#以下の2文章にtokenizerを適用する\n",
    "text='''Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? \n",
    "They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't \n",
    "remove the template from the talk page since I'm retired now.89.205.38.2'''\n",
    "\n",
    "text2='''\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics \n",
    "should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so \n",
    "that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - \n",
    "if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere \n",
    "appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed \n",
    "in the relevant form eg Wikipedia:Good_article_nominations#Transport  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJyjTU73-VAe",
    "outputId": "ca9ebc39-f2c3-4802-8006-105db4465298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explanation', 'why', 'the', 'edit', '##s', 'made', 'under', 'my', 'user', '##name', 'hardcore', 'metallic', '##a', 'fan', 'were', 'reverted', '?', 'they', 'weren', \"'\", 't', 'van', '##dal', '##isms', ',', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fa', '##c', '.', 'and', 'please', 'don', \"'\", 't', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'i', \"'\", 'm', 'retired', 'now', '.', '89', '.', '205', '.', '38', '.', '2']\n"
     ]
    }
   ],
   "source": [
    "#tokenizeで前処理結果が見れる\n",
    "#前とのつながりを表す##が特徴的\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9qYqUaF-U7u",
    "outputId": "d1fae910-503a-4f54-88fe-3978be90aa56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7526, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310, 18442, 13076, 12392, 2050, 5470, 2020, 16407, 1029, 2027, 4694, 1005, 1056, 3158, 9305, 22556, 1010, 2074, 8503, 2006, 2070, 3806, 2044, 1045, 5444, 2012, 2047, 2259, 14421, 6904, 2278, 1012, 1998, 3531, 2123, 1005, 1056, 6366, 1996, 23561, 2013, 1996, 2831, 3931, 2144, 1045, 1005, 1049, 3394, 2085, 1012, 6486, 1012, 16327, 1012, 4229, 1012, 1016, 102]\n"
     ]
    }
   ],
   "source": [
    "#encodeで数値に変換可能\n",
    "tokenized_sentence = tokenizer.encode(\n",
    "  text,                      #Sentence to encode.\n",
    "  add_special_tokens = True, #[CLS]と[SEP]を加える\n",
    "  max_length = max_seq_len,  #最大長さを指定\n",
    "  )    \n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0EEkpjm-2CD",
    "outputId": "f44f0fe4-f4c7-49f1-97cf-c2c68afbd605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101⇒[CLS]\n",
      "7526⇒explanation\n",
      "2339⇒why\n",
      "1996⇒the\n",
      "10086⇒edit\n",
      "2015⇒##s\n",
      "2081⇒made\n",
      "2104⇒under\n",
      "2026⇒my\n",
      "5310⇒user\n"
     ]
    }
   ],
   "source": [
    "#各数字がどの単語かを確認\n",
    "#文頭と文末にそれぞれ[CLS],[SEP]が追加されている\n",
    "for input_id in tokenized_sentence[:10]:\n",
    "    print('{}⇒{}'.format(input_id, tokenizer.decode([input_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZP9lcLw_-cj"
   },
   "outputs": [],
   "source": [
    "#上記と同様に2つめの文章もencode\n",
    "tokenized_sentence2 = tokenizer.encode(\n",
    "  text2,                      #Sentence to encode.\n",
    "  add_special_tokens = True, #[CLS]と[SEP]を加える\n",
    "  max_length = max_seq_len,  #最大長さを指定\n",
    "  )    \n",
    "\n",
    "tokenized_sentence_list=[]\n",
    "tokenized_sentence_list.append(tokenized_sentence)\n",
    "tokenized_sentence_list.append(tokenized_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcDHcQUt_rVt",
    "outputId": "e7fa12c2-9fa4-4eb6-efcc-0aa998f6c954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  7526  2339  1996 10086  2015  2081  2104  2026  5310 18442 13076\n",
      "  12392  2050  5470  2020 16407  1029  2027  4694  1005  1056  3158  9305\n",
      "  22556  1010  2074  8503  2006  2070  3806  2044  1045  5444  2012  2047\n",
      "   2259 14421  6904  2278  1012  1998  3531  2123  1005  1056  6366  1996\n",
      "  23561  2013  1996  2831  3931  2144  1045  1005  1049  3394  2085  1012\n",
      "   6486  1012 16327  1012  4229  1012  1016   102     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  101  2062  1045  2064  1005  1056  2191  2151  2613 15690  2006  7620\n",
      "   1011  1045  4999  2065  1996  2930  6747  2323  2022  2101  2006  1010\n",
      "   2030  1037  4942 29015  1997  1000  1000  4127  1997 13436  1000  1000\n",
      "   1011  1045  2228  1996  7604  2089  2342 29369  2075  2061  2008  2027\n",
      "   2024  2035  1999  1996  6635  2168  4289 29464  3058  4289  4385  1012\n",
      "   1045  2064  2079  2008  2101  2006  1010  2065  2053  1011  2028  2842\n",
      "   2515  2034  1011  2065  2017  2031  2151 18394  2005  4289  3436  2806\n",
      "   2006  7604  2030  2215  2000  2079  2009  4426  3531  2292  2033  2113\n",
      "   1012  2045  3544  2000  2022  1037  2067 21197  2006  4790  2005  3319\n",
      "   2061  1045  3984  2045  2089  2022  1037  8536  2127  1037 12027  4332\n",
      "   2039  1012  2009  1005  1055  3205  1999   102]]\n"
     ]
    }
   ],
   "source": [
    "#pad_sequencesでndarrayに変換される\n",
    "#長い方に合わせて0でpaddingされる\n",
    "\n",
    "tokenized_and_padded_sentences=pad_sequences(tokenized_sentence_list, maxlen=max_seq_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "print(tokenized_and_padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEuMWKrZFEzH"
   },
   "source": [
    "### 3.1.2 attention_mask(パディング位置)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrYPBTM6BdK1",
    "outputId": "2170e2ea-cd08-4bea-fe37-01ba0391c6f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#0よりも大きければattention_masks=1とする\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in tokenized_and_padded_sentences:\n",
    "  att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "  attention_masks.append(att_mask)\n",
    "\n",
    "print(np.asarray(attention_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqOqgF7yCVxS"
   },
   "source": [
    "## 3.2 [3.1]の処理を全データに適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ket3UsPROef-",
    "outputId": "7e6c2e2b-e7d7-4ce0-85a2-c81a4d55d3b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [04:37<00:00, 574.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#時間がかかるので一部で試行\n",
    "#df_train=df_train.head(20000)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "                            sentence,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                    )\n",
    "        \n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "\n",
    "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "attention_masks = create_attention_masks(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kcO1RHtDVz8"
   },
   "source": [
    "# 3.3 データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkdZ-usqQKD-",
    "outputId": "73d04f5b-7b3f-43a7-a0ea-58a5ca87c003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143613\n",
      "15958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ラベルの6列をlabelsに格納\n",
    "labels =  df_train[label_cols].values\n",
    "\n",
    "#trainとtestに分割\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
    "\n",
    "#attention_maskもtrain用を作成\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
    "\n",
    "train_size = len(train_inputs)\n",
    "validation_size = len(validation_inputs)\n",
    "\n",
    "print(train_size)\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMOVvDNDF-Rk"
   },
   "source": [
    "## 3.4 token_type_idsの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au6dYuSZbdWs",
    "outputId": "bcd1c5a9-4f89-44cd-80c5-e96cbfec823e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ファインチューニングの場合はすべて0とする\n",
    "token_type_ids=np.zeros(train_inputs.shape)\n",
    "token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4fEFDLvGQvs"
   },
   "source": [
    "## 3.5 input_ids, attention_masks, token_type_idsを一つのリストに格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L31N6RUQopDF",
    "outputId": "aad6af28-3d28-4354-b1f1-ab9ea9b08f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[  101  1045  2293  5980  2015  1999  2026 10007  1012  1006  1031  1031\n",
      "  5310   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "attention_masks:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "token_type_ids:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train=[]\n",
    "x_train.append(train_inputs)\n",
    "x_train.append(train_masks)\n",
    "x_train.append(token_type_ids)\n",
    "\n",
    "#データの確認\n",
    "print('input_ids:')\n",
    "print(x_train[0][1])\n",
    "print('attention_masks:')\n",
    "print(x_train[1][1])\n",
    "print('token_type_ids:')\n",
    "print(x_train[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE1eC-1sH-r7"
   },
   "source": [
    "# 4. Validationデータの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rUBR9tbdEML",
    "outputId": "2c05b215-fb1a-4303-f6fd-ffa4eb14ca4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[ 101 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065\n",
      " 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534\n",
      " 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045\n",
      " 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999\n",
      " 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545\n",
      "  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065\n",
      " 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534\n",
      " 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999 1045\n",
      " 8534 7065 2545  999 1045 8534 7065 2545  999 1045 8534 7065 2545  999\n",
      " 1045  102]\n",
      "attention_masks:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "token_type_ids:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#同じことをvalidationデータでも実施\n",
    "token_type_ids=np.zeros(validation_inputs.shape)\n",
    "token_type_ids\n",
    "\n",
    "x_val=[]\n",
    "x_val.append(validation_inputs)\n",
    "x_val.append(validation_masks)\n",
    "x_val.append(token_type_ids)\n",
    "\n",
    "#データの確認\n",
    "print('input_ids:')\n",
    "print(x_val[0][1])\n",
    "print('attention_masks:')\n",
    "print(x_val[1][1])\n",
    "print('token_type_ids:')\n",
    "print(x_val[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n778MlTIJVj"
   },
   "source": [
    "# 5. モデルの作成、学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9iwzolDoEtk"
   },
   "outputs": [],
   "source": [
    "#Epoch終わりで、AUCを評価するためのクラスを定義する\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__() #Callbackのinitメソッドを呼び出し\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #intervalで割り切れるエポックの時のみ\n",
    "        if epoch % self.interval == 0:\n",
    "            #validationデータを評価して、AUCを出力する\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            print(\"toxic Confusion matrix=====================================\")\n",
    "            print(confusion_matrix(self.y_val[:,0], y_pred[:,0]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,0], y_pred[:,0]))\n",
    "\n",
    "            print(\"severe_toxic Confusion matrix==============================\")\n",
    "            print(confusion_matrix(self.y_val[:,1], y_pred[:,1]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,1], y_pred[:,1]))\n",
    "\n",
    "            print(\"obscene Confusion matrix===================================\")\n",
    "            print(confusion_matrix(self.y_val[:,2], y_pred[:,2]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,2], y_pred[:,2]))\n",
    "\n",
    "            print(\"threat Confusion matrix====================================\")\n",
    "            print(confusion_matrix(self.y_val[:,3], y_pred[:,3]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,3], y_pred[:,3]))\n",
    "\n",
    "            print(\"insult Confusion matrix====================================\")\n",
    "            print(confusion_matrix(self.y_val[:,4], y_pred[:,4]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,4], y_pred[:,4]))\n",
    "\n",
    "            print(\"identity_hate Confusion matrix=============================\")\n",
    "            print(confusion_matrix(self.y_val[:,5], y_pred[:,5]>0.5))\n",
    "            print(\"ROC-AUC:\",roc_auc_score(self.y_val[:,5], y_pred[:,5]))\n",
    "            print('')\n",
    "            print('')\n",
    "\n",
    "#AUCクラスをインスタンス化\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_val,validation_labels), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBKdyPh3VwiL",
    "outputId": "f2e3f068-70a6-4c13-aa15-9f21732ab327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   ((None, 128, 768), ( 109482240   input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            4614        tf_bert_model_1[0][1]            \n",
      "==================================================================================================\n",
      "Total params: 109,486,854\n",
      "Trainable params: 109,486,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "4488/4488 [==============================] - ETA: 0s - loss: 0.0453 - acc: 0.8905\n",
      " ROC-AUC - epoch: 1 - score: 0.989757 \n",
      "\n",
      "toxic Confusion matrix=====================================\n",
      "[[14267   160]\n",
      " [  313  1218]]\n",
      "ROC-AUC: 0.9886678295743923\n",
      "severe_toxic Confusion matrix==============================\n",
      "[[15751    46]\n",
      " [  120    41]]\n",
      "ROC-AUC: 0.990012255648824\n",
      "obscene Confusion matrix===================================\n",
      "[[15015    85]\n",
      " [  186   672]]\n",
      "ROC-AUC: 0.9935149508328316\n",
      "threat Confusion matrix====================================\n",
      "[[15909     6]\n",
      " [   29    14]]\n",
      "ROC-AUC: 0.9947760267116732\n",
      "insult Confusion matrix====================================\n",
      "[[14991   154]\n",
      " [  213   600]]\n",
      "ROC-AUC: 0.9881951305482022\n",
      "identity_hate Confusion matrix=============================\n",
      "[[15779    32]\n",
      " [   85    62]]\n",
      "ROC-AUC: 0.983377627820466\n",
      "\n",
      "\n",
      "4488/4488 [==============================] - 6822s 2s/step - loss: 0.0453 - acc: 0.8905 - val_loss: 0.0375 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81180c9208>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "#input_shapeを指定\n",
    "input_shape = (128, )\n",
    "\n",
    "#num_classes=6クラス\n",
    "num_classes=len(label_cols)\n",
    "\n",
    "#modelの作成\n",
    "model_name='bert-base-uncased'\n",
    "input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "bert_model = transformers.TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "#以下のように、転移学習も可能\n",
    "#bert_model.trainable = False\n",
    "\n",
    "#last_hidden_state（モデルの最後の隠れ状態）、pooler_output(CLSと呼ばれる文章の平均値)、\n",
    "#hidden_states(全隠れ状態)、attentions(Attentionレイヤの出力)が入っているのでpooler_outputを文章の要約として全結合層に渡す\n",
    "last_hidden_state, pooler_output = bert_model(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids\n",
    "    )\n",
    "\n",
    "#Activationはsigmoid\n",
    "output = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")(pooler_output)\n",
    "\n",
    "#モデルをコンパイル\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-05, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "\n",
    "#モデルを確認\n",
    "print(model.summary())\n",
    "\n",
    "# 訓練\n",
    "model.fit(\n",
    "    x_train,\n",
    "    train_labels,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    validation_data=(x_val,validation_labels),\n",
    "    callbacks=[RocAuc],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYgZsN8TH6mF"
   },
   "source": [
    "# 6. 推論の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NmgUqO6yycc"
   },
   "outputs": [],
   "source": [
    "#推論を実施\n",
    "#df_test=df_test.head(2000)\n",
    "#df_submission=df_submission.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICoD1yxFqMjH",
    "outputId": "b2cd6b7f-65e6-4da8-a004-3126578307c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [04:15<00:00, 599.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#学習データと同様の前処理を実行\n",
    "test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "test_attention_masks = create_attention_masks(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEvCOBC5q6vG",
    "outputId": "78e92a2d-364a-4f10-a7ce-2ef2fbb69dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#token_type_idsの作成\n",
    "token_type_ids=np.zeros(test_attention_masks.shape)\n",
    "token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Osv3twcZqnVE",
    "outputId": "e02b61d6-9c26-44ad-c715-59c324f48022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 128)\n"
     ]
    }
   ],
   "source": [
    "#テストデータの作成\n",
    "x_test=[]\n",
    "x_test.append(test_input_ids)\n",
    "x_test.append(test_attention_masks)\n",
    "x_test.append(token_type_ids)\n",
    "\n",
    "print(x_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km6y9IKyoz0y",
    "outputId": "e5d1ab3b-46bc-4ce4-e96e-e5a8834a5615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787/4787 [==============================] - 2300s 480ms/step\n"
     ]
    }
   ],
   "source": [
    "#予測\n",
    "y_pred = model.predict(x_test, batch_size=32,verbose=1)\n",
    "df_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "#df_submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "qZS9ymIrzrRe",
    "outputId": "fb6d88e3-db8d-4242-e9b4-4329d1b8c761"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>0.989021</td>\n",
       "      <td>0.349818</td>\n",
       "      <td>0.938313</td>\n",
       "      <td>0.099160</td>\n",
       "      <td>0.869715</td>\n",
       "      <td>0.610838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffcd0960ee309b5</th>\n",
       "      <td>0.482821</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.160274</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.029287</td>\n",
       "      <td>0.002017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffd7a9a6eb32c16</th>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffda9e8d6fafa9e</th>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffe8f1340a79fc2</th>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffce3fb183ee80</th>\n",
       "      <td>0.944353</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>0.683713</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.392403</td>\n",
       "      <td>0.009584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     toxic  severe_toxic  ...    insult  identity_hate\n",
       "id                                        ...                         \n",
       "00001cee341fdb12  0.989021      0.349818  ...  0.869715       0.610838\n",
       "0000247867823ef7  0.001209      0.000051  ...  0.000084       0.000052\n",
       "00013b17ad220c46  0.000665      0.000064  ...  0.000046       0.000075\n",
       "00017563c3f7919a  0.000174      0.000065  ...  0.000046       0.000088\n",
       "00017695ad8997eb  0.000697      0.000052  ...  0.000056       0.000040\n",
       "...                    ...           ...  ...       ...            ...\n",
       "fffcd0960ee309b5  0.482821      0.001680  ...  0.029287       0.002017\n",
       "fffd7a9a6eb32c16  0.008160      0.000078  ...  0.000378       0.000111\n",
       "fffda9e8d6fafa9e  0.000345      0.000052  ...  0.000048       0.000049\n",
       "fffe8f1340a79fc2  0.000971      0.000069  ...  0.000096       0.000234\n",
       "ffffce3fb183ee80  0.944353      0.026552  ...  0.392403       0.009584\n",
       "\n",
       "[153164 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#結果の格納\n",
    "df_submission.to_csv('submission1.csv', index=True)\n",
    "df_submission"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle Toxic Comment Bert",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
